{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!yes | apt-get update --allow-releaseinfo-change\n",
    "!yes | apt install libgoogle-perftools-dev\n",
    "!git clone https://github.com/logn-2024/Any2anyTryon\n",
    "%cd Any2anyTryon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"--- Installing from requirements.txt ---\")\n",
    "!pip install -r requirements.txt -q\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# --- Authentication ---\n",
    "hf_token = \"YOUR_HF_API_KEY\" # Replace with your actual token\n",
    "\n",
    "# Login using the token\n",
    "try:\n",
    "    login(token=hf_token)\n",
    "    print(\"Hugging Face login successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Hugging Face login failed: {e}\")\n",
    "    # Handle the error appropriately, maybe exit or raise\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install pyngrok\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Check the output of the previous cell to confirm the Gradio port\n",
    "gradio_port = 7860  # Or the port your Gradio app is running on\n",
    "\n",
    "# Optional: Set your ngrok authtoken for more stable tunnels\n",
    "ngrok.set_auth_token(\"YOUR_NGROQ_API_KEY\")\n",
    "\n",
    "try:\n",
    "    public_url = ngrok.connect(gradio_port)\n",
    "    print(f\"Gradio interface is available at: {public_url}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to ngrok: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "from diffusers import FluxTransformer2DModel, AutoencoderKL\n",
    "from diffusers.hooks import apply_group_offloading\n",
    "from transformers import T5EncoderModel, CLIPTextModel\n",
    "from src.pipeline_tryon import FluxTryonPipeline\n",
    "from optimum.quanto import freeze, qfloat8, quantize\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "#torch_dtype = torch.bfloat16 # torch.float16\n",
    "torch_dtype = torch.float16\n",
    "\n",
    "def load_models(device=device, torch_dtype=torch_dtype, group_offloading=False):\n",
    "    \"\"\"\n",
    "    Loads the Any2Any models with optimizations, corrected offloading hook order,\n",
    "    and low_cpu_mem_usage to reduce RAM spikes during loading.\n",
    "\n",
    "    Args:\n",
    "        device (torch.device): The target device (e.g., 'cuda').\n",
    "        torch_dtype (torch.dtype): The data type (should be torch.float16 for P100/T4).\n",
    "        group_offloading (bool): Whether to use group offloading instead of\n",
    "                                 enable_model_cpu_offload.\n",
    "\n",
    "    Returns:\n",
    "        FluxTryonPipeline: The loaded and configured pipeline.\n",
    "    \"\"\"\n",
    "    print(f\"--- Loading models ---\")\n",
    "    print(f\"Target device: {device}\")\n",
    "    print(f\"Using dtype: {torch_dtype}\")\n",
    "    print(f\"Group Offloading Flag: {group_offloading}\")\n",
    "\n",
    "    \n",
    "\n",
    "    bfl_repo = \"black-forest-labs/FLUX.1-dev\"\n",
    "\n",
    "    # --- Load Model Components with low_cpu_mem_usage ---\n",
    "    # This argument helps reduce peak CPU RAM usage during loading\n",
    "    print(\"Loading text_encoder...\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\n",
    "        bfl_repo,\n",
    "        subfolder=\"text_encoder\",\n",
    "        torch_dtype=torch_dtype,\n",
    "        low_cpu_mem_usage=True  # <<< ADDED/CORRECTED HERE\n",
    "    )\n",
    "    print(\"Loading text_encoder_2...\")\n",
    "    text_encoder_2 = T5EncoderModel.from_pretrained(\n",
    "        bfl_repo,\n",
    "        subfolder=\"text_encoder_2\",\n",
    "        torch_dtype=torch_dtype,\n",
    "        low_cpu_mem_usage=True  # <<< ADDED/CORRECTED HERE\n",
    "    )\n",
    "    try:\n",
    "        import bitsandbytes # Check if available\n",
    "        load_in_4bit = True\n",
    "        print(\"Attempting to load transformer directly in 4-bit...\")\n",
    "    except ImportError:\n",
    "        print(\"bitsandbytes not found, cannot load transformer in 4-bit directly.\")\n",
    "        load_in_4bit = False\n",
    "    print(\"Loading transformer...\")\n",
    "    transformer = FluxTransformer2DModel.from_pretrained(\n",
    "        bfl_repo,\n",
    "        subfolder=\"transformer\",\n",
    "        torch_dtype=torch_dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        low_cpu_mem_usage=True  # <<< ADDED/CORRECTED HERE\n",
    "    )\n",
    "    print(\"Loading vae...\")\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        bfl_repo,\n",
    "        subfolder=\"vae\",\n",
    "        torch_dtype=torch_dtype,\n",
    "        low_cpu_mem_usage=True  # <<< ADDED/CORRECTED HERE\n",
    "    )\n",
    "\n",
    "    # --- Create Pipeline ---\n",
    "    print(\"Creating FluxTryonPipeline...\")\n",
    "    pipe = FluxTryonPipeline.from_pretrained(\n",
    "        bfl_repo, # Base repo info might still be needed by pipeline\n",
    "        transformer=transformer,\n",
    "        text_encoder=text_encoder,\n",
    "        text_encoder_2=text_encoder_2,\n",
    "        vae=vae,\n",
    "        torch_dtype=torch_dtype,\n",
    "        # Note: The pipeline itself usually doesn't take low_cpu_mem_usage,\n",
    "        # it uses the components already loaded efficiently.\n",
    "    )\n",
    "\n",
    "    # --- Apply Quantization ---\n",
    "    print(\"Quantizing text_encoder_2 (qfloat8)...\")\n",
    "    try:\n",
    "        quantize(pipe.text_encoder_2, weights=qfloat8)\n",
    "        freeze(pipe.text_encoder_2)\n",
    "    except NameError:\n",
    "        print(\"Skipping quantization as optimum.quanto was not imported.\")\n",
    "\n",
    "    # --- Apply Other Optimizations ---\n",
    "    print(\"Enabling slicing optimizations...\")\n",
    "    pipe.enable_attention_slicing()\n",
    "    pipe.vae.enable_slicing()\n",
    "    pipe.vae.enable_tiling()\n",
    "\n",
    "    # --- Fix Offloading Hook Order ---\n",
    "\n",
    "    # 1. Remove any existing hooks FIRST\n",
    "    print(\"Removing existing pipeline hooks (if any)...\")\n",
    "    pipe.remove_all_hooks()\n",
    "\n",
    "    # 2. Load LoRA weights (after removing hooks, before adding new offload hooks)\n",
    "    print(\"Loading LoRA weights...\")\n",
    "    try:\n",
    "        pipe.load_lora_weights(\n",
    "            \"loooooong/Any2anyTryon\",\n",
    "            weight_name=\"dev_lora_any2any_alltasks.safetensors\",\n",
    "            adapter_name=\"tryon\",\n",
    "        )\n",
    "        print(\"LoRA weights loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to load LoRA weights - {e}\")\n",
    "\n",
    "    # 3. Enable the desired offloading mechanism. Choose ONE.\n",
    "    if not group_offloading:\n",
    "        # Default path: Use the simpler enable_model_cpu_offload\n",
    "        print(\"Applying enable_model_cpu_offload...\")\n",
    "        pipe.enable_model_cpu_offload()\n",
    "        print(\"CPU offload enabled via enable_model_cpu_offload.\")\n",
    "    else:\n",
    "        # Optional path: Use group offloading if the flag is set\n",
    "        print(\"Applying group_offloading (will not use enable_model_cpu_offload)...\")\n",
    "        apply_group_offloading(\n",
    "            pipe.transformer,\n",
    "            offload_type=\"leaf_level\",\n",
    "            offload_device=torch.device(\"cpu\"),\n",
    "            onload_device=torch.device(device),\n",
    "            use_stream=True,\n",
    "        )\n",
    "        apply_group_offloading(\n",
    "            pipe.text_encoder,\n",
    "            offload_device=torch.device(\"cpu\"),\n",
    "            onload_device=torch.device(device),\n",
    "            offload_type=\"leaf_level\",\n",
    "            use_stream=True,\n",
    "        )\n",
    "        # Apply to text_encoder_2 as well if needed\n",
    "        # apply_group_offloading(\n",
    "        #     pipe.text_encoder_2,\n",
    "        #     offload_device=torch.device(\"cpu\"),\n",
    "        #     onload_device=torch.device(device),\n",
    "        #     offload_type=\"leaf_level\",\n",
    "        #     use_stream=True,\n",
    "        # )\n",
    "        apply_group_offloading(\n",
    "            pipe.vae,\n",
    "            offload_device=torch.device(\"cpu\"),\n",
    "            onload_device=torch.device(device),\n",
    "            offload_type=\"leaf_level\",\n",
    "            use_stream=True,\n",
    "        )\n",
    "        print(\"Group offloading applied.\")\n",
    "\n",
    "    # 4. REMOVE the final pipe.to(device) call. Offloading handles device placement.\n",
    "    # print(\"Skipping final pipe.to(device) because offloading is enabled.\")\n",
    "\n",
    "    print(\"--- Model loading and setup complete ---\")\n",
    "    return pipe\n",
    "\n",
    "def crop_to_multiple_of_16(img):\n",
    "    width, height = img.size\n",
    "    \n",
    "    # Calculate new dimensions that are multiples of 8\n",
    "    new_width = width - (width % 16)  \n",
    "    new_height = height - (height % 16)\n",
    "    \n",
    "    # Calculate crop box coordinates\n",
    "    left = (width - new_width) // 2\n",
    "    top = (height - new_height) // 2\n",
    "    right = left + new_width\n",
    "    bottom = top + new_height\n",
    "    \n",
    "    # Crop the image\n",
    "    cropped_img = img.crop((left, top, right, bottom))\n",
    "    \n",
    "    return cropped_img\n",
    "\n",
    "def resize_and_pad_to_size(image, target_width, target_height):\n",
    "    # Convert numpy array to PIL Image if needed\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = Image.fromarray(image)\n",
    "        \n",
    "    # Get original dimensions\n",
    "    orig_width, orig_height = image.size\n",
    "    \n",
    "    # Calculate aspect ratios\n",
    "    target_ratio = target_width / target_height\n",
    "    orig_ratio = orig_width / orig_height\n",
    "    \n",
    "    # Calculate new dimensions while maintaining aspect ratio\n",
    "    if orig_ratio > target_ratio:\n",
    "        # Image is wider than target ratio - scale by width\n",
    "        new_width = target_width\n",
    "        new_height = int(new_width / orig_ratio)\n",
    "    else:\n",
    "        # Image is taller than target ratio - scale by height\n",
    "        new_height = target_height\n",
    "        new_width = int(new_height * orig_ratio)\n",
    "        \n",
    "    # Resize image\n",
    "    resized_image = image.resize((new_width, new_height))\n",
    "    \n",
    "    # Create white background image of target size\n",
    "    padded_image = Image.new('RGB', (target_width, target_height), 'white')\n",
    "    \n",
    "    # Calculate padding to center the image\n",
    "    left_padding = (target_width - new_width) // 2\n",
    "    top_padding = (target_height - new_height) // 2\n",
    "    \n",
    "    # Paste resized image onto padded background\n",
    "    padded_image.paste(resized_image, (left_padding, top_padding))\n",
    "    \n",
    "    return padded_image, left_padding, top_padding, target_width - new_width - left_padding, target_height - new_height - top_padding\n",
    "\n",
    "def resize_by_height(image, height):\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = Image.fromarray(image)\n",
    "    # image is a PIL image\n",
    "    image = image.resize((int(image.width * height / image.height), height))\n",
    "    return crop_to_multiple_of_16(image)\n",
    "\n",
    "# @spaces.GPU()\n",
    "@torch.no_grad\n",
    "def generate_image(prompt, model_image, garment_image, height=256, width=256, seed=0, guidance_scale=3.5, show_type=\"follow model image\", num_inference_steps=30):\n",
    "    height, width = int(height), int(width)\n",
    "    width = width - (width % 16)  \n",
    "    height = height - (height % 16)\n",
    "\n",
    "    concat_image_list = [np.zeros((height, width, 3), dtype=np.uint8)]\n",
    "    has_model_image = model_image is not None\n",
    "    has_garment_image = garment_image is not None\n",
    "    if has_model_image:\n",
    "        if has_garment_image:\n",
    "            # if both model and garment image are provided, ensure model image and target image have the same size\n",
    "            input_height, input_width = model_image.shape[:2]\n",
    "            model_image, lp, tp, rp, bp = resize_and_pad_to_size(Image.fromarray(model_image), width, height)\n",
    "        else:\n",
    "            model_image = resize_by_height(model_image, height)\n",
    "        # model_image = resize_and_pad_to_size(Image.fromarray(model_image), width, height)\n",
    "        concat_image_list.append(model_image)\n",
    "    if has_garment_image:\n",
    "        # if has_model_image:\n",
    "        #     garment_image = resize_and_pad_to_size(Image.fromarray(garment_image), width, height)\n",
    "        # else:\n",
    "        garment_image = resize_by_height(garment_image, height)\n",
    "        concat_image_list.append(garment_image)\n",
    "\n",
    "    image = np.concatenate([np.array(img) for img in concat_image_list], axis=1)\n",
    "    image = Image.fromarray(image)\n",
    "    \n",
    "    mask = np.zeros_like(image)\n",
    "    mask[:,:width] = 255\n",
    "    mask_image = Image.fromarray(mask)\n",
    "    \n",
    "    assert height==image.height, \"ensure same height\"\n",
    "    # with torch.cuda.amp.autocast(): # this cause black image\n",
    "    # with torch.no_grad():\n",
    "    output = pipe(\n",
    "        prompt,\n",
    "        image=image,\n",
    "        mask_image=mask_image,\n",
    "        strength=1.,\n",
    "        height=height,\n",
    "        width=image.width,\n",
    "        target_width=width,\n",
    "        tryon=has_model_image and has_garment_image,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        max_sequence_length=512,\n",
    "        generator=torch.Generator().manual_seed(seed),\n",
    "        output_type=\"latent\",\n",
    "    ).images\n",
    "    \n",
    "    latents = pipe._unpack_latents(output, image.height, image.width, pipe.vae_scale_factor)\n",
    "    if show_type!=\"all outputs\":\n",
    "        latents = latents[:,:,:,:width//pipe.vae_scale_factor]\n",
    "    latents = (latents / pipe.vae.config.scaling_factor) + pipe.vae.config.shift_factor\n",
    "    image = pipe.vae.decode(latents, return_dict=False)[0]\n",
    "    image = pipe.image_processor.postprocess(image, output_type=\"pil\")[0]\n",
    "    output = image\n",
    "    if show_type==\"follow model image\" and has_model_image and has_garment_image:\n",
    "        output = output.crop((lp, tp, output.width-rp, output.height-bp)).resize((input_width, input_height))\n",
    "    \n",
    "    return output\n",
    "\n",
    "def update_dimensions(model_image, garment_image, height, width, auto_ar):\n",
    "    if not auto_ar:\n",
    "        return height, width\n",
    "    if model_image is not None:\n",
    "        height = model_image.shape[0]\n",
    "        width = model_image.shape[1]\n",
    "    elif garment_image is not None:\n",
    "        height = garment_image.shape[0]\n",
    "        width = garment_image.shape[1]\n",
    "    else:\n",
    "        height = 512\n",
    "        width = 384\n",
    "\n",
    "    # Set max dimensions and minimum size\n",
    "    max_height = 1024\n",
    "    max_width = 1024\n",
    "    min_size = 384\n",
    "\n",
    "    # Scale down if exceeds max dimensions while maintaining aspect ratio\n",
    "    if height > max_height or width > max_width:\n",
    "        aspect_ratio = width / height\n",
    "        if height > max_height:\n",
    "            height = max_height\n",
    "            width = int(height * aspect_ratio)\n",
    "        if width > max_width:\n",
    "            width = max_width\n",
    "            height = int(width / aspect_ratio)\n",
    "\n",
    "    # Scale up if below minimum size while maintaining aspect ratio\n",
    "    if height < min_size and width < min_size:\n",
    "        aspect_ratio = width / height\n",
    "        if height < width:\n",
    "            height = min_size\n",
    "            width = int(height * aspect_ratio)\n",
    "        else:\n",
    "            width = min_size\n",
    "            height = int(width / aspect_ratio)\n",
    "\n",
    "    return height, width\n",
    "\n",
    "model1 = Image.open(\"asset/images/model/model1.png\") \n",
    "model2 = Image.open(\"asset/images/model/model2.jpg\")\n",
    "model3 = Image.open(\"asset/images/model/model3.png\") \n",
    "model4 = Image.open(\"asset/images/model/model4.png\")\n",
    "\n",
    "garment1 = Image.open(\"asset/images/garment/garment1.jpg\") \n",
    "garment2 = Image.open(\"asset/images/garment/garment2.jpg\")\n",
    "garment3 = Image.open(\"asset/images/garment/garment3.jpg\") \n",
    "garment4 = Image.open(\"asset/images/garment/garment4.jpg\")\n",
    "\n",
    "def launch_demo():\n",
    "    with gr.Blocks() as demo:   \n",
    "        gr.Markdown(\"# Any2AnyTryon\")\n",
    "        gr.Markdown(\"Demo(experimental) for [Any2AnyTryon: Leveraging Adaptive Position Embeddings for Versatile Virtual Clothing Tasks](https://arxiv.org/abs/2501.15891) ([Code](https://github.com/logn-2024/Any2anyTryon)).\") \n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                model_image = gr.Image(label=\"Model Image\", type=\"numpy\", interactive=True,)\n",
    "                with gr.Row():\n",
    "                    garment_image = gr.Image(label=\"Garment Image\", type=\"numpy\", interactive=True,)\n",
    "                    with gr.Column():\n",
    "                        prompt = gr.Textbox(\n",
    "                            label=\"Prompt\",\n",
    "                            info=\"Try example prompts from right side\",\n",
    "                            placeholder=\"Enter your prompt here...\",\n",
    "                            value=\"\",\n",
    "                            # visible=False,\n",
    "                        )\n",
    "                        with gr.Row():\n",
    "                            height = gr.Number(label=\"Height\", value=256, precision=0)\n",
    "                            width = gr.Number(label=\"Width\", value=256, precision=0)\n",
    "                        seed = gr.Number(label=\"Seed\", value=0, precision=0)\n",
    "                        with gr.Accordion(\"Advanced Settings\", open=False):\n",
    "                            guidance_scale = gr.Number(label=\"Guidance Scale\", value=3.5)\n",
    "                            num_inference_steps = gr.Number(label=\"Inference Steps\", value=15)\n",
    "                            show_type = gr.Radio(label=\"Show Type\",choices=[\"follow model image\", \"follow height & width\", \"all outputs\"],value=\"follow model image\")\n",
    "                            auto_ar = gr.Checkbox(label=\"Detect Image Size(From Uploaded Images)\", value=False, visible=True,)\n",
    "                btn = gr.Button(\"Generate\")\n",
    "            \n",
    "            with gr.Column():\n",
    "                output = gr.Image(label=\"Generated Image\")\n",
    "                example_prompts = gr.Examples(\n",
    "                        [\n",
    "                            \"<MODEL> a person with fashion garment. <GARMENT> a garment. <TARGET> model with fashion garment\",\n",
    "                            \"<MODEL> a person with fashion garment. <TARGET> the same garment laid flat.\",\n",
    "                            \"<GARMENT> The image shows a fashion garment. <TARGET> a smiling person with the garment in white background\",\n",
    "                        ],\n",
    "                        inputs=prompt,\n",
    "                        label=\"Example Prompts\",\n",
    "                        # visible=False\n",
    "                    )\n",
    "                example_model = gr.Examples(\n",
    "                    examples=[\n",
    "                        model1, model2, model3, model4\n",
    "                    ],\n",
    "                    inputs=model_image,\n",
    "                    label=\"Example Model Images\"\n",
    "                )\n",
    "                example_garment = gr.Examples(\n",
    "                    examples=[\n",
    "                        garment1, garment2, garment3, garment4\n",
    "                    ],\n",
    "                    inputs=garment_image,\n",
    "                    label=\"Example Garment Images\"\n",
    "                )\n",
    "\n",
    "        # Update dimensions when images change\n",
    "        model_image.change(fn=update_dimensions, \n",
    "                        inputs=[model_image, garment_image, height, width, auto_ar],\n",
    "                        outputs=[height, width])\n",
    "        garment_image.change(fn=update_dimensions,\n",
    "                            inputs=[model_image, garment_image, height, width, auto_ar], \n",
    "                            outputs=[height, width])    \n",
    "        btn.click(fn=generate_image,\n",
    "                inputs=[prompt, model_image, garment_image, height, width, seed, guidance_scale, show_type, num_inference_steps],\n",
    "                outputs=output)\n",
    "\n",
    "        demo.title = \"FLUX Image Generation Demo\"\n",
    "        demo.description = \"Generate images using FLUX model with LoRA\"\n",
    "        \n",
    "        examples = [\n",
    "            # tryon\n",
    "            [\n",
    "                '''<MODEL> a man <GARMENT> a medium-sized, short-sleeved, blue t-shirt with a round neckline and a pocket on the front. <TARGET> model with fashion garment''',\n",
    "                model1,\n",
    "                garment1,\n",
    "                576, 576\n",
    "            ],\n",
    "            [\n",
    "                '''<MODEL> a man with gray hair and a beard wearing a black jacket and sunglasses, standing in front of a body of water with mountains in the background and a cloudy sky above <GARMENT> a black and white striped t-shirt with a red heart embroidered on the chest <TARGET> ''',\n",
    "                model2,\n",
    "                garment2,\n",
    "                576, 576\n",
    "            ],\n",
    "            [\n",
    "                '''<MODEL> a person with fashion garment. <GARMENT> a garment. <TARGET> model with fashion garment''',\n",
    "                model3,\n",
    "                garment3,\n",
    "                576, 576\n",
    "            ],\n",
    "            [\n",
    "                '''<MODEL> a woman lift up her right leg. <GARMENT> a pair of black and white patterned pajama pants. <TARGET> model with fashion garment''',\n",
    "                model4,\n",
    "                garment4,\n",
    "                576, 576\n",
    "            ],\n",
    "        ]\n",
    "        \n",
    "        gr.Examples(\n",
    "            examples=examples,\n",
    "            inputs=[prompt, model_image, garment_image],\n",
    "            outputs=output,\n",
    "            fn=generate_image,\n",
    "            cache_examples=False,\n",
    "            examples_per_page=20\n",
    "        )\n",
    "    demo.queue().launch(share=False, show_error=False,\n",
    "        server_name=\"0.0.0.0\"\n",
    "    )\n",
    "if __name__ == \"__main__\":\n",
    "    # Using parse_known_args to avoid issues with notebook args\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--group_offloading', action=\"store_true\", help=\"Use group offloading instead of enable_model_cpu_offload\")\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    group_offload_setting = args.group_offloading\n",
    "\n",
    "    print(f\"Starting model loading with group_offloading={group_offload_setting}\")\n",
    "    # Make sure your custom pipeline and quanto are available\n",
    "    try:\n",
    "        # If you haven't uploaded 'src' yet, this will fail\n",
    "        from src.pipeline_tryon import FluxTryonPipeline\n",
    "        # If optimum.quanto is not installed, this will fail (or use placeholder)\n",
    "        from optimum.quanto import freeze, qfloat8, quantize\n",
    "        loaded_pipe = load_models(group_offloading=group_offload_setting)\n",
    "        print(\"Pipeline ready.\")\n",
    "        launch_demo() # Or your demo function call\n",
    "    except ModuleNotFoundError as e:\n",
    "        print(f\"ERROR: Required module not found: {e}\")\n",
    "        print(\"Please ensure 'src' directory and 'optimum.quanto' are available/installed.\")\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"An error occurred during loading:\")\n",
    "        traceback.print_exc() # Print full traceback for better debugging"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
