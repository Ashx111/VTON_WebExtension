{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Zheng-Chong/CatVTON\n",
    "%cd CatVTON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q \\\n",
    " \"torch>=2.1.0,<2.5.0\" \\\n",
    " \"torchvision>=0.16.0,<0.20.0\" \\\n",
    " accelerate>=0.30.0 \\\n",
    " diffusers>=0.27.0 \\\n",
    " matplotlib>=3.8.0 \\\n",
    " numpy>=1.25.0 \\\n",
    " opencv_python>=4.9.0 \\\n",
    " pillow>=10.0.0 \\\n",
    " PyYAML>=6.0 \\\n",
    " scipy>=1.11.0 \\\n",
    " scikit-image>=0.22.0 \\\n",
    " tqdm>=4.66.0 \\\n",
    " transformers>=4.40.0 \\\n",
    " fvcore>=0.1.5 \\\n",
    " cloudpickle>=3.0.0 \\\n",
    " omegaconf>=2.3.0 \\\n",
    " pycocotools>=2.0.7 \\\n",
    " av>=12.0.0 \\\n",
    " gradio>=4.25.0 \\\n",
    " peft>=0.11.0 \\\n",
    " huggingface_hub>=0.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install pyngrok\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Check the output of the previous cell to confirm the Gradio port\n",
    "gradio_port = 7860  # Or the port your Gradio app is running on\n",
    "\n",
    "# Optional: Set your ngrok authtoken for more stable tunnels\n",
    "ngrok.set_auth_token(\"YOUR_NGROQ_API_KEY\")\n",
    "\n",
    "try:\n",
    "    public_url = ngrok.connect(gradio_port)\n",
    "    print(f\"Gradio interface is available at: {public_url}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to ngrok: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# --- Authentication ---\n",
    "hf_token = \"YOUR_HF_API_KEY\" # Replace with your actual token\n",
    "\n",
    "# Login using the token\n",
    "try:\n",
    "    login(token=hf_token)\n",
    "    print(\"Hugging Face login successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Hugging Face login failed: {e}\")\n",
    "    # Handle the error appropriately, maybe exit or raise\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "import gradio as gr\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from huggingface_hub import snapshot_download\n",
    "from PIL import Image\n",
    "import traceback # For printing full errors\n",
    "\n",
    "# --- Check and Import Local Files ---\n",
    "# This script assumes 'model' and 'utils' directories are present\n",
    "# in the same location as this script in your Kaggle environment.\n",
    "try:\n",
    "    from model.cloth_masker import AutoMasker, vis_mask\n",
    "    from model.flux.pipeline_flux_tryon import FluxTryOnPipeline\n",
    "    from utils import resize_and_crop, resize_and_padding\n",
    "    print(\"Successfully imported local modules: cloth_masker, pipeline_flux_tryon, utils\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"ERROR: Failed to import local modules: {e}\")\n",
    "    print(\"Please ensure the 'model' and 'utils' directories containing the necessary .py files\")\n",
    "    print(\"are uploaded to the same directory as this script in your Kaggle environment.\")\n",
    "    # Optional: Exit if local files are crucial and missing\n",
    "    # exit()\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during local module import: {e}\")\n",
    "    # exit()\n",
    "\n",
    "# --- Configuration (Replaces argparse) ---\n",
    "class AppConfig:\n",
    "    # Correct Hugging Face repo ID (verify if FLUX.1-Fill-dev exists under this org)\n",
    "    base_model_path = \"black-forest-labs/FLUX.1-Fill-dev\"\n",
    "    # CatVTON repo for LoRA and auxiliary models\n",
    "    resume_path = \"zhengchong/CatVTON\"\n",
    "    output_dir = \"resource/demo/output\"\n",
    "    # Use FP16 for P100/T4 compatibility and reduced memory\n",
    "    mixed_precision = \"fp16\"\n",
    "    allow_tf32 = False # TF32 not relevant/supported on P100\n",
    "    width = 768 # Target resolution width\n",
    "    height = 1024 # Target resolution height\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch_dtype = torch.float16 if mixed_precision == \"fp16\" else (torch.bfloat16 if mixed_precision == \"bf16\" else torch.float32)\n",
    "\n",
    "args = AppConfig() # Create an object to hold config values\n",
    "\n",
    "print(\"--- Configuration ---\")\n",
    "print(f\"Base Model: {args.base_model_path}\")\n",
    "print(f\"Resume Path: {args.resume_path}\")\n",
    "print(f\"Target Device: {args.device}\")\n",
    "print(f\"Target dtype: {args.torch_dtype}\")\n",
    "print(f\"Target Resolution: {args.width}x{args.height}\")\n",
    "print(\"--------------------\")\n",
    "\n",
    "# --- Global Variables for Models (Load Once) ---\n",
    "pipeline_flux = None\n",
    "automasker = None\n",
    "mask_processor = None\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def image_grid(imgs, rows, cols):\n",
    "    if not imgs: return None # Handle empty list\n",
    "    if len(imgs) != rows * cols:\n",
    "         print(f\"Warning: image_grid expected {rows*cols} images, got {len(imgs)}\")\n",
    "         # Handle mismatch, e.g., create a blank grid or adjust rows/cols\n",
    "         # For now, just return None or the first image\n",
    "         return imgs[0] if imgs else Image.new(\"RGB\", (100,100))\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "    for i, img in enumerate(imgs):\n",
    "        # Ensure img is a PIL Image object\n",
    "        if not isinstance(img, Image.Image):\n",
    "             print(f\"Warning: Item {i} is not a PIL image, skipping.\")\n",
    "             continue # Or create a placeholder\n",
    "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "    return grid\n",
    "\n",
    "# --- Model Loading Function (Called Once) ---\n",
    "def load_all_models_and_tools():\n",
    "    global pipeline_flux, automasker, mask_processor\n",
    "    print(\"--- Starting Model Loading ---\")\n",
    "    try:\n",
    "        # Download CatVTON specific files (LoRA, DensePose, SCHP)\n",
    "        print(f\"Downloading CatVTON resume files from: {args.resume_path}\")\n",
    "        repo_path = snapshot_download(repo_id=args.resume_path)\n",
    "        print(f\"CatVTON files downloaded to: {repo_path}\")\n",
    "\n",
    "        # Load the base FLUX pipeline\n",
    "        # Using low_cpu_mem_usage to reduce RAM spike during initial loading\n",
    "        print(f\"Loading FLUX pipeline from: {args.base_model_path}\")\n",
    "        pipeline_flux = FluxTryOnPipeline.from_pretrained(\n",
    "            args.base_model_path,\n",
    "            torch_dtype=args.torch_dtype,\n",
    "            low_cpu_mem_usage=True # Reduce CPU RAM usage\n",
    "        )\n",
    "        print(\"FLUX pipeline loaded.\")\n",
    "\n",
    "        # Load the CatVTON LoRA weights specific for FLUX\n",
    "        print(\"Loading CatVTON LoRA weights for FLUX...\")\n",
    "        lora_path = os.path.join(repo_path, \"flux-lora\")\n",
    "        lora_weight_file = os.path.join(lora_path, 'pytorch_lora_weights.safetensors')\n",
    "        if os.path.exists(lora_path) and os.path.exists(lora_weight_file):\n",
    "             pipeline_flux.load_lora_weights(\n",
    "                 lora_path,\n",
    "                 weight_name='pytorch_lora_weights.safetensors'\n",
    "             )\n",
    "             print(\"LoRA weights loaded.\")\n",
    "        else:\n",
    "             print(f\"Warning: LoRA path or weight file not found at {lora_path}. Skipping LoRA.\")\n",
    "\n",
    "\n",
    "        # Move pipeline to GPU *before* enabling offload\n",
    "        print(f\"Moving pipeline to {args.device} with dtype {args.torch_dtype}...\")\n",
    "        pipeline_flux.to(args.device) # No dtype here, already loaded with it\n",
    "        print(\"Pipeline moved to device.\")\n",
    "\n",
    "        # Enable CPU offloading *after* moving to device and loading LoRA\n",
    "        print(\"Enabling CPU model offload...\")\n",
    "        pipeline_flux.enable_model_cpu_offload()\n",
    "        print(\"CPU offload enabled.\")\n",
    "\n",
    "        # Initialize AutoMasker\n",
    "        print(\"Initializing AutoMasker...\")\n",
    "        mask_processor = VaeImageProcessor(\n",
    "            vae_scale_factor=pipeline_flux.vae_scale_factor if hasattr(pipeline_flux, 'vae_scale_factor') else 8, # Use pipeline's scale factor if available\n",
    "            do_normalize=False,\n",
    "            do_binarize=True,\n",
    "            do_convert_grayscale=True\n",
    "        )\n",
    "        densepose_path = os.path.join(repo_path, \"DensePose\")\n",
    "        schp_path = os.path.join(repo_path, \"SCHP\")\n",
    "        if not os.path.exists(densepose_path): print(f\"Warning: DensePose path not found at {densepose_path}\")\n",
    "        if not os.path.exists(schp_path): print(f\"Warning: SCHP path not found at {schp_path}\")\n",
    "\n",
    "        automasker = AutoMasker(\n",
    "            densepose_ckpt=densepose_path,\n",
    "            schp_ckpt=schp_path,\n",
    "            device=args.device # Load masker models to the GPU\n",
    "        )\n",
    "        print(\"AutoMasker initialized.\")\n",
    "        print(\"--- Model Loading Complete ---\")\n",
    "\n",
    "    except ImportError as e:\n",
    "         print(f\"ERROR during loading: Failed to import a required module: {e}\")\n",
    "         print(\"This likely means 'model' or 'utils' directories are missing or incorrect.\")\n",
    "         raise # Re-raise the error to stop execution\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during loading: An unexpected error occurred.\")\n",
    "        traceback.print_exc() # Print the full error details\n",
    "        raise # Re-raise the error\n",
    "\n",
    "# --- Gradio Submit Function ---\n",
    "def submit_function_flux(\n",
    "    person_image_input, # Renamed to avoid conflict with variable inside\n",
    "    cloth_image_input, # Renamed\n",
    "    cloth_type,\n",
    "    num_inference_steps,\n",
    "    guidance_scale,\n",
    "    seed,\n",
    "    show_type\n",
    "):\n",
    "    # Ensure models are loaded\n",
    "    if pipeline_flux is None or automasker is None or mask_processor is None:\n",
    "        return \"ERROR: Models not loaded. Please check loading logs.\", None # Return error message\n",
    "\n",
    "    print(\"--- Starting Inference ---\")\n",
    "    try:\n",
    "        # Process image editor input\n",
    "        # Check if input is a dictionary (from ImageEditor) or just a path (from Examples)\n",
    "        if isinstance(person_image_input, dict) and \"background\" in person_image_input:\n",
    "             person_image_path = person_image_input[\"background\"]\n",
    "             mask_path = person_image_input[\"layers\"][0] if person_image_input[\"layers\"] else None\n",
    "        elif isinstance(person_image_input, str): # Path from examples\n",
    "             person_image_path = person_image_input\n",
    "             mask_path = None # No mask from examples\n",
    "        else:\n",
    "             raise ValueError(\"Invalid person image input format\")\n",
    "\n",
    "        if isinstance(cloth_image_input, str):\n",
    "            cloth_image_path = cloth_image_input\n",
    "        else:\n",
    "             raise ValueError(\"Invalid cloth image input format\")\n",
    "\n",
    "        person_image = Image.open(person_image_path).convert(\"RGB\")\n",
    "        cloth_image = Image.open(cloth_image_path).convert(\"RGB\")\n",
    "\n",
    "        mask = None\n",
    "        if mask_path is not None:\n",
    "            print(\"Processing provided mask...\")\n",
    "            mask_pil = Image.open(mask_path).convert(\"L\")\n",
    "            # Check if mask is empty (all black or all white)\n",
    "            mask_np = np.array(mask_pil)\n",
    "            if len(np.unique(mask_np)) > 1 and np.any(mask_np > 128): # Check if not empty and has non-black pixels\n",
    "                 mask_np[mask_np > 0] = 255 # Binarize\n",
    "                 mask = Image.fromarray(mask_np)\n",
    "                 print(\"Using user-drawn mask.\")\n",
    "            else:\n",
    "                 print(\"User-drawn mask is empty or invalid, will generate automatically.\")\n",
    "                 mask = None # Fallback to auto-mask\n",
    "\n",
    "\n",
    "        # Adjust image sizes\n",
    "        print(f\"Resizing images to {args.width}x{args.height}\")\n",
    "        person_image_resized = resize_and_crop(person_image, (args.width, args.height))\n",
    "        cloth_image_resized = resize_and_padding(cloth_image, (args.width, args.height))\n",
    "\n",
    "        # Process mask (Generate if not provided or invalid)\n",
    "        if mask is not None:\n",
    "            print(\"Resizing provided mask...\")\n",
    "            mask = resize_and_crop(mask, (args.width, args.height))\n",
    "        else:\n",
    "            print(f\"Generating mask for type: {cloth_type}\")\n",
    "            mask_data = automasker(person_image_resized, cloth_type)\n",
    "            if mask_data and 'mask' in mask_data:\n",
    "                 mask = mask_data['mask']\n",
    "                 print(\"Auto-mask generated.\")\n",
    "            else:\n",
    "                 print(\"Warning: Auto-mask generation failed. Using blank mask.\")\n",
    "                 # Create a blank (black) mask as fallback\n",
    "                 mask = Image.new(\"L\", (args.width, args.height), 0)\n",
    "\n",
    "        print(\"Blurring mask...\")\n",
    "        mask = mask_processor.blur(mask, blur_factor=9)\n",
    "\n",
    "        # Set random seed\n",
    "        generator = None\n",
    "        if seed != -1:\n",
    "            print(f\"Using seed: {seed}\")\n",
    "            generator = torch.Generator(device=args.device).manual_seed(seed)\n",
    "        else:\n",
    "            print(\"Using random seed.\")\n",
    "\n",
    "\n",
    "        # Inference\n",
    "        print(f\"Running pipeline inference with {num_inference_steps} steps, CFG {guidance_scale}...\")\n",
    "        with torch.inference_mode(): # Ensure gradients are off\n",
    "             with torch.autocast(args.device, dtype=args.torch_dtype, enabled=(args.mixed_precision != \"no\")): # Enable autocast\n",
    "                result_image = pipeline_flux(\n",
    "                    image=person_image_resized,\n",
    "                    condition_image=cloth_image_resized,\n",
    "                    mask_image=mask,\n",
    "                    height=args.height,\n",
    "                    width=args.width,\n",
    "                    num_inference_steps=int(num_inference_steps),\n",
    "                    guidance_scale=float(guidance_scale),\n",
    "                    generator=generator\n",
    "                ).images[0]\n",
    "        print(\"Inference complete.\")\n",
    "\n",
    "        # Post-processing\n",
    "        print(\"Creating visualization...\")\n",
    "        masked_person_vis = vis_mask(person_image_resized, mask) # Use resized person image\n",
    "\n",
    "        # Return result based on show type\n",
    "        if show_type == \"result only\":\n",
    "            print(\"Displaying result only.\")\n",
    "            return result_image\n",
    "        else:\n",
    "            output_width, output_height = result_image.size # Should match args.width, args.height\n",
    "            if show_type == \"input & result\":\n",
    "                print(\"Displaying input & result.\")\n",
    "                condition_width = output_width // 2\n",
    "                conditions = image_grid([person_image_resized, cloth_image_resized], 2, 1)\n",
    "            else: # \"input & mask & result\"\n",
    "                print(\"Displaying input & mask & result.\")\n",
    "                condition_width = output_width // 3\n",
    "                conditions = image_grid([person_image_resized, masked_person_vis, cloth_image_resized], 1, 3) # Grid requires matching dims\n",
    "\n",
    "            if conditions is None:\n",
    "                 print(\"Warning: Condition grid failed, returning result only.\")\n",
    "                 return result_image\n",
    "\n",
    "            conditions_resized = conditions.resize((condition_width, output_height), Image.LANCZOS) # Use LANCZOS for better quality\n",
    "            new_result_image = Image.new(\"RGB\", (output_width + condition_width + 5, output_height))\n",
    "            new_result_image.paste(conditions_resized, (0, 0))\n",
    "            new_result_image.paste(result_image, (condition_width + 5, 0))\n",
    "            print(\"Combined visualization created.\")\n",
    "            return new_result_image\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during inference:\")\n",
    "        traceback.print_exc()\n",
    "        # Return error message to Gradio interface\n",
    "        return f\"Error during inference: {e}\", None\n",
    "\n",
    "\n",
    "# --- Gradio UI Definition ---\n",
    "def app_gradio():\n",
    "    global args # Allow access to config\n",
    "\n",
    "    with gr.Blocks(title=\"CatVTON with FLUX\") as demo:\n",
    "        gr.Markdown(\"# CatVTON with FLUX\") # Removed specific model name for flexibility\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=350):\n",
    "                with gr.Row():\n",
    "                    # Hidden image path holder for examples\n",
    "                    image_path_flux = gr.Image(\n",
    "                        type=\"filepath\", interactive=False, visible=False\n",
    "                    )\n",
    "                    # Image Editor for user input and mask drawing\n",
    "                    person_image_flux = gr.ImageEditor(\n",
    "                         label=\"Person Image (Draw Mask Here if Needed)\",\n",
    "                         sources=[\"upload\", \"clipboard\"], # Allow upload and paste\n",
    "                         type=\"filepath\", # Saves uploaded/edited image to a temp path\n",
    "                         interactive=True,\n",
    "                         # Tool order might matter for default selection\n",
    "                         # tools=[\"select\", \"move\", \"sketch\"], # Default sketch tool? Check Gradio docs\n",
    "                         brush=gr.Brush(default_size=20, colors=[\"#FFFFFF\"], color_mode=\"fixed\") # White brush for mask\n",
    "                    )\n",
    "\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1, min_width=230):\n",
    "                        cloth_image_flux = gr.Image(\n",
    "                            interactive=True, label=\"Garment/Condition Image\",\n",
    "                            sources=[\"upload\", \"clipboard\"], type=\"filepath\"\n",
    "                        )\n",
    "                    with gr.Column(scale=1, min_width=120):\n",
    "                        gr.Markdown(\n",
    "                            '<span style=\"color: #808080; font-size: small;\">Provide Mask via:<br>1. `üñåÔ∏è` Draw on Person image<br>2. Auto-generate below</span>'\n",
    "                        )\n",
    "                        cloth_type = gr.Radio(\n",
    "                            label=\"Auto-Mask Cloth Type\",\n",
    "                            info=\"Used if no mask is drawn above\",\n",
    "                            choices=[\"upper\", \"lower\", \"overall\"],\n",
    "                            value=\"upper\",\n",
    "                        )\n",
    "\n",
    "                submit_flux = gr.Button(\"Submit Try-On\")\n",
    "                gr.Markdown(\n",
    "                    '<center><span style=\"color: #FF0000\">Wait few minutes after clicking Submit!</span></center>'\n",
    "                )\n",
    "\n",
    "                with gr.Accordion(\"Advanced Options\", open=False):\n",
    "                    num_inference_steps_flux = gr.Slider(\n",
    "                        label=\"Inference Steps\", minimum=10, maximum=100, step=1, value=30 # Reduced default steps\n",
    "                    )\n",
    "                    guidance_scale_flux = gr.Slider(\n",
    "                        label=\"Guidance Scale (CFG)\", minimum=0.0, maximum=10.0, step=0.1, value=2.5 # Reduced default/max CFG\n",
    "                    )\n",
    "                    seed_flux = gr.Slider(\n",
    "                        label=\"Seed (-1 for random)\", minimum=-1, maximum=2147483647, step=1, value=42\n",
    "                    )\n",
    "                    show_type = gr.Radio(\n",
    "                        label=\"Output Display\",\n",
    "                        choices=[\"result only\", \"input & result\", \"input & mask & result\"],\n",
    "                        value=\"input & mask & result\",\n",
    "                    )\n",
    "\n",
    "            with gr.Column(scale=2, min_width=500):\n",
    "                result_image_flux = gr.Image(interactive=False, label=\"Result\", type=\"pil\") # Output as PIL\n",
    "                with gr.Row():\n",
    "                    # Define example paths *after* args is defined\n",
    "                    root_path = \"resource/demo/example\" # Make sure this path exists in Kaggle\n",
    "                    person_example_path = os.path.join(root_path, \"person\")\n",
    "                    condition_example_path = os.path.join(root_path, \"condition\")\n",
    "\n",
    "                    with gr.Column():\n",
    "                        # Check if example dirs exist before creating Examples\n",
    "                        if os.path.exists(os.path.join(person_example_path, \"men\")):\n",
    "                            gr.Examples(\n",
    "                                examples=[os.path.join(person_example_path, \"men\", f) for f in os.listdir(os.path.join(person_example_path, \"men\")) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],\n",
    "                                examples_per_page=4,\n",
    "                                inputs=image_path_flux, # Link to hidden image path component\n",
    "                                label=\"Person Examples (Men)\",\n",
    "                            )\n",
    "                        if os.path.exists(os.path.join(person_example_path, \"women\")):\n",
    "                             gr.Examples(\n",
    "                                examples=[os.path.join(person_example_path, \"women\", f) for f in os.listdir(os.path.join(person_example_path, \"women\")) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],\n",
    "                                examples_per_page=4,\n",
    "                                inputs=image_path_flux, # Link to hidden image path component\n",
    "                                label=\"Person Examples (Women)\",\n",
    "                             )\n",
    "                        gr.Markdown('<span style=\"color: #808080; font-size: small;\">*Example images may need to be uploaded to `resource/demo/example`</span>')\n",
    "\n",
    "                    with gr.Column():\n",
    "                        if os.path.exists(os.path.join(condition_example_path, \"upper\")):\n",
    "                             gr.Examples(\n",
    "                                examples=[os.path.join(condition_example_path, \"upper\", f) for f in os.listdir(os.path.join(condition_example_path, \"upper\")) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],\n",
    "                                examples_per_page=4,\n",
    "                                inputs=cloth_image_flux,\n",
    "                                label=\"Condition Upper Examples\",\n",
    "                             )\n",
    "                        if os.path.exists(os.path.join(condition_example_path, \"overall\")):\n",
    "                             gr.Examples(\n",
    "                                examples=[os.path.join(condition_example_path, \"overall\", f) for f in os.listdir(os.path.join(condition_example_path, \"overall\")) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],\n",
    "                                examples_per_page=4,\n",
    "                                inputs=cloth_image_flux,\n",
    "                                label=\"Condition Overall Examples\",\n",
    "                             )\n",
    "                        # Add more condition examples if needed...\n",
    "\n",
    "\n",
    "        # --- Event Handlers ---\n",
    "        # When an example person image is clicked, update the hidden path holder\n",
    "        # Then, use a .then() event to update the actual ImageEditor from the path holder\n",
    "        # This prevents examples directly overwriting ImageEditor mask layers\n",
    "        def update_editor_from_path(filepath):\n",
    "            print(f\"Loading example image into editor: {filepath}\")\n",
    "            if filepath and os.path.exists(filepath):\n",
    "                 # Return dict format expected by ImageEditor: background only\n",
    "                 return {\"background\": filepath, \"layers\": [], \"composite\": None}\n",
    "            return None # Return None if path is invalid\n",
    "\n",
    "        # Link example clicks to the hidden component\n",
    "        # Find all gr.Examples components targeting image_path_flux if dynamically generated\n",
    "        example_components_person = [comp for comp in demo.GetComponents() if isinstance(comp, gr.Examples) and comp.inputs == [image_path_flux]]\n",
    "        for ex_comp in example_components_person:\n",
    "             ex_comp.click(\n",
    "                 fn=lambda x: x, # Pass the filepath through\n",
    "                 inputs=ex_comp.inputs,\n",
    "                 outputs=image_path_flux # Update the hidden path holder\n",
    "             ).then(\n",
    "                 fn=update_editor_from_path,\n",
    "                 inputs=image_path_flux, # Read from the hidden path holder\n",
    "                 outputs=person_image_flux # Update the ImageEditor\n",
    "             )\n",
    "\n",
    "        # Submit button click\n",
    "        submit_flux.click(\n",
    "            fn=submit_function_flux,\n",
    "            inputs=[person_image_flux, cloth_image_flux, cloth_type, num_inference_steps_flux, guidance_scale_flux, seed_flux, show_type],\n",
    "            outputs=result_image_flux, # Only output the final image/result grid\n",
    "            api_name=\"catvton_flux_tryon\" # Add API name if needed\n",
    "        )\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Load models only once when the script starts\n",
    "    load_all_models_and_tools()\n",
    "\n",
    "    # Create and launch the Gradio app\n",
    "    gradio_app = app_gradio()\n",
    "    gradio_app.queue().launch(share=False, show_error=True) # share=True if you want a public link from Kaggle"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
